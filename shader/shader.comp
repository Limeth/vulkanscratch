#version 450
#extension GL_ARB_gpu_shader_int64 : require

// {{{ secp256k1

// {{{ secp256k1 macros
#define SECP256K1_FE_CONST(d7, d6, d5, d4, d3, d2, d1, d0) secp256k1_fe(uint[10](\
    (d0) & 0x3FFFFFF, \
    ((d0) >> 26) | (((d1) & 0xFFFFF) << 6), \
    ((d1) >> 20) | (((d2) & 0x3FFF) << 12), \
    ((d2) >> 14) | (((d3) & 0xFF) << 18), \
    ((d3) >> 8) | (((d4) & 0x3) << 24), \
    ((d4) >> 2) & 0x3FFFFFF, \
    ((d4) >> 28) | (((d5) & 0x3FFFFF) << 4), \
    ((d5) >> 22) | (((d6) & 0xFFFF) << 10), \
    ((d6) >> 16) | (((d7) & 0x3FF) << 16), \
    ((d7) >> 10)\
))
// }}}

// {{{ secp256k1 structs
struct secp256k1_scalar {
    uint[8] d;
};

struct secp256k1_fe {
   uint[10] n;
};

struct secp256k1_gej {
    secp256k1_fe x;
    secp256k1_fe y;
    secp256k1_fe z;
    int infinity;
};

struct secp256k1_ge {
    secp256k1_fe x;
    secp256k1_fe y;
    int infinity;
};

struct secp256k1_fe_storage {
    uint[8] n;
};

struct secp256k1_ge_storage {
    secp256k1_fe_storage x;
    secp256k1_fe_storage y;
};

struct secp256k1_ecmult_gen_context {
    secp256k1_ge_storage[64][16] prec;  // (*prec);
    secp256k1_scalar blind;
    secp256k1_gej initial;
};

struct secp256k1_ecmult_gen_context_part_prec_quarter {
    secp256k1_ge_storage[16][16] array_quarter;  // (*prec);
};

struct secp256k1_ecmult_gen_context_part_rest {
    secp256k1_scalar blind;
    secp256k1_gej initial;
};

// secp256k1_context_struct == secp256k1_context
struct secp256k1_context_struct {
    /* secp256k1_ecmult_context ecmult_ctx; */
    /* secp256k1_ecmult_gen_context ecmult_gen_ctx; */
    secp256k1_ecmult_gen_context ctx;
    /* secp256k1_callback illegal_callback; */
    /* secp256k1_callback error_callback; */
};
// }}}

// {{{ secp256k1 constants
/* Limbs of the secp256k1 order. */
const uint SECP256K1_N_0 = 0xD0364141;
const uint SECP256K1_N_1 = 0xBFD25E8C;
const uint SECP256K1_N_2 = 0xAF48A03B;
const uint SECP256K1_N_3 = 0xBAAEDCE6;
const uint SECP256K1_N_4 = 0xFFFFFFFE;
const uint SECP256K1_N_5 = 0xFFFFFFFF;
const uint SECP256K1_N_6 = 0xFFFFFFFF;
const uint SECP256K1_N_7 = 0xFFFFFFFF;

/* Limbs of 2^256 minus the secp256k1 order. */
const uint SECP256K1_N_C_0 = (~SECP256K1_N_0 + 1);
const uint SECP256K1_N_C_1 = (~SECP256K1_N_1);
const uint SECP256K1_N_C_2 = (~SECP256K1_N_2);
const uint SECP256K1_N_C_3 = (~SECP256K1_N_3);
const uint SECP256K1_N_C_4 = (1);

/* Limbs of half the secp256k1 order. */
const uint SECP256K1_N_H_0 = 0x681B20A0;
const uint SECP256K1_N_H_1 = 0xDFE92F46;
const uint SECP256K1_N_H_2 = 0x57A4501D;
const uint SECP256K1_N_H_3 = 0x5D576E73;
const uint SECP256K1_N_H_4 = 0xFFFFFFFF;
const uint SECP256K1_N_H_5 = 0xFFFFFFFF;
const uint SECP256K1_N_H_6 = 0xFFFFFFFF;
const uint SECP256K1_N_H_7 = 0x7FFFFFFF;
// }}}

// {{{ secret key verification
bool secp256k1_scalar_check_overflow(in const secp256k1_scalar a) {
    bool yes = false;
    bool no = false;
    no  = no  || (a.d[7] < SECP256K1_N_7); /* No need for a > check. */
    no  = no  || (a.d[6] < SECP256K1_N_6); /* No need for a > check. */
    no  = no  || (a.d[5] < SECP256K1_N_5); /* No need for a > check. */
    no  = no  || (a.d[4] < SECP256K1_N_4);
    yes = yes || (a.d[4] > SECP256K1_N_4) && !no;
    no  = no  || (a.d[3] < SECP256K1_N_3) && !yes;
    yes = yes || (a.d[3] > SECP256K1_N_3) && !no;
    no  = no  || (a.d[2] < SECP256K1_N_2) && !yes;
    yes = yes || (a.d[2] > SECP256K1_N_2) && !no;
    no  = no  || (a.d[1] < SECP256K1_N_1) && !yes;
    yes = yes || (a.d[1] > SECP256K1_N_1) && !no;
    yes = yes || (a.d[0] >= SECP256K1_N_0) && !no;
    return yes;
}

int secp256k1_scalar_reduce(in out secp256k1_scalar r, in bool overflow_bool) {
    int overflow = int(overflow_bool);
    uint t;
    t = r.d[0] + overflow * SECP256K1_N_C_0;
    r.d[0] = t & 0xFFFFFFFF; t >>= 32;
    t += r.d[1] + overflow * SECP256K1_N_C_1;
    r.d[1] = t & 0xFFFFFFFF; t >>= 32;
    t += r.d[2] + overflow * SECP256K1_N_C_2;
    r.d[2] = t & 0xFFFFFFFF; t >>= 32;
    t += r.d[3] + overflow * SECP256K1_N_C_3;
    r.d[3] = t & 0xFFFFFFFF; t >>= 32;
    t += r.d[4] + overflow * SECP256K1_N_C_4;
    r.d[4] = t & 0xFFFFFFFF; t >>= 32;
    t += r.d[5];
    r.d[5] = t & 0xFFFFFFFF; t >>= 32;
    t += r.d[6];
    r.d[6] = t & 0xFFFFFFFF; t >>= 32;
    t += r.d[7];
    r.d[7] = t & 0xFFFFFFFF;
    return overflow;
}

int secp256k1_scalar_set_b32(out secp256k1_scalar r, in uint[32] b32) {
    r.d[0] = b32[31] | b32[30] << 8 | b32[29] << 16 | b32[28] << 24;
    r.d[1] = b32[27] | b32[26] << 8 | b32[25] << 16 | b32[24] << 24;
    r.d[2] = b32[23] | b32[22] << 8 | b32[21] << 16 | b32[20] << 24;
    r.d[3] = b32[19] | b32[18] << 8 | b32[17] << 16 | b32[16] << 24;
    r.d[4] = b32[15] | b32[14] << 8 | b32[13] << 16 | b32[12] << 24;
    r.d[5] = b32[11] | b32[10] << 8 | b32[9] << 16 | b32[8] << 24;
    r.d[6] = b32[7] | b32[6] << 8 | b32[5] << 16 | b32[4] << 24;
    r.d[7] = b32[3] | b32[2] << 8 | b32[1] << 16 | b32[0] << 24;
    return secp256k1_scalar_reduce(r, secp256k1_scalar_check_overflow(r));
}

bool secp256k1_scalar_is_zero(in secp256k1_scalar a) {
    return (a.d[0] | a.d[1] | a.d[2] | a.d[3] | a.d[4] | a.d[5] | a.d[6] | a.d[7]) == 0;
}

void secp256k1_scalar_clear(in out secp256k1_scalar r) {
    r.d[0] = 0;
    r.d[1] = 0;
    r.d[2] = 0;
    r.d[3] = 0;
    r.d[4] = 0;
    r.d[5] = 0;
    r.d[6] = 0;
    r.d[7] = 0;
}

bool secp256k1_ec_seckey_verify(in uint[32] seckey) {
    secp256k1_scalar sec;
    int overflow = secp256k1_scalar_set_b32(sec, seckey);
    return !bool(overflow) && !secp256k1_scalar_is_zero(sec);
}

bool secp256k1_ec_seckey_verify(in secp256k1_scalar sec) {
    int overflow = secp256k1_scalar_reduce(sec, secp256k1_scalar_check_overflow(sec));
    return !bool(overflow) && !bool(secp256k1_scalar_is_zero(sec));
}
// }}}

// {{{ public key creation
int secp256k1_scalar_add(out secp256k1_scalar r, in secp256k1_scalar a, in secp256k1_scalar b) {
    int overflow;
    uint t = a.d[0] + b.d[0];
    r.d[0] = t & 0xFFFFFFFF; t >>= 32;
    t += a.d[1] + b.d[1];
    r.d[1] = t & 0xFFFFFFFF; t >>= 32;
    t += a.d[2] + b.d[2];
    r.d[2] = t & 0xFFFFFFFF; t >>= 32;
    t += a.d[3] + b.d[3];
    r.d[3] = t & 0xFFFFFFFF; t >>= 32;
    t += a.d[4] + b.d[4];
    r.d[4] = t & 0xFFFFFFFF; t >>= 32;
    t += a.d[5] + b.d[5];
    r.d[5] = t & 0xFFFFFFFF; t >>= 32;
    t += a.d[6] + b.d[6];
    r.d[6] = t & 0xFFFFFFFF; t >>= 32;
    t += a.d[7] + b.d[7];
    r.d[7] = t & 0xFFFFFFFF; t >>= 32;
    overflow = int(t) + int(secp256k1_scalar_check_overflow(r));
    secp256k1_scalar_reduce(r, bool(overflow));
    return overflow;
}

uint secp256k1_scalar_get_bits(in const secp256k1_scalar a, in const uint offset, in const uint count) {
    return (a.d[offset >> 5] >> (offset & 0x1F)) & ((1 << count) - 1);
}

void secp256k1_fe_storage_cmov(
    in out secp256k1_fe_storage r,
    in const secp256k1_fe_storage a,
    in const bool flag
) {
    uint mask0, mask1;
    mask0 = int(flag) + ~0;
    mask1 = ~mask0;
    r.n[0] = (r.n[0] & mask0) | (a.n[0] & mask1);
    r.n[1] = (r.n[1] & mask0) | (a.n[1] & mask1);
    r.n[2] = (r.n[2] & mask0) | (a.n[2] & mask1);
    r.n[3] = (r.n[3] & mask0) | (a.n[3] & mask1);
    r.n[4] = (r.n[4] & mask0) | (a.n[4] & mask1);
    r.n[5] = (r.n[5] & mask0) | (a.n[5] & mask1);
    r.n[6] = (r.n[6] & mask0) | (a.n[6] & mask1);
    r.n[7] = (r.n[7] & mask0) | (a.n[7] & mask1);
}

void secp256k1_ge_storage_cmov(
    in out secp256k1_ge_storage r,
    in const secp256k1_ge_storage a,
    in const bool flag
) {
    secp256k1_fe_storage_cmov(r.x, a.x, flag);
    secp256k1_fe_storage_cmov(r.y, a.y, flag);
}

void secp256k1_fe_from_storage(in out secp256k1_fe r, in const secp256k1_fe_storage a) {
    r.n[0] = a.n[0] & 0x3FFFFFF;
    r.n[1] = a.n[0] >> 26 | ((a.n[1] << 6) & 0x3FFFFFF);
    r.n[2] = a.n[1] >> 20 | ((a.n[2] << 12) & 0x3FFFFFF);
    r.n[3] = a.n[2] >> 14 | ((a.n[3] << 18) & 0x3FFFFFF);
    r.n[4] = a.n[3] >> 8 | ((a.n[4] << 24) & 0x3FFFFFF);
    r.n[5] = (a.n[4] >> 2) & 0x3FFFFFF;
    r.n[6] = a.n[4] >> 28 | ((a.n[5] << 4) & 0x3FFFFFF);
    r.n[7] = a.n[5] >> 22 | ((a.n[6] << 10) & 0x3FFFFFF);
    r.n[8] = a.n[6] >> 16 | ((a.n[7] << 16) & 0x3FFFFFF);
    r.n[9] = a.n[7] >> 10;
}

void secp256k1_ge_from_storage(in out secp256k1_ge r, in const secp256k1_ge_storage a) {
    secp256k1_fe_from_storage(r.x, a.x);
    secp256k1_fe_from_storage(r.y, a.y);
    r.infinity = 0;
}

void secp256k1_fe_set_int(in out secp256k1_fe r, in const int a) {
    r.n[0] = a;
    r.n[1] = r.n[2] = r.n[3] = r.n[4] = r.n[5] = r.n[6] = r.n[7] = r.n[8] = r.n[9] = 0;
}

void secp256k1_gej_set_ge(in out secp256k1_gej r, in const secp256k1_ge a) {
   r.infinity = a.infinity;
   r.x = a.x;
   r.y = a.y;
   secp256k1_fe_set_int(r.z, 1);
}

/* void secp256k1_gej_add_ge_var( */
/*         in out secp256k1_gej r, */
/*         in const secp256k1_gej a, */
/*         in const secp256k1_ge b, */
/*         in out secp256k1_fe rzr */
/* ) { */
/*     /1* 8 mul, 3 sqr, 4 normalize, 12 mul_int/add/negate *1/ */
/*     secp256k1_fe z12, u1, u2, s1, s2, h, i, i2, h2, h3, t; */

/*     if (a.infinity != 0) { */
/*         /1* VERIFY_CHECK(rzr == NULL); *1/ */
/*         secp256k1_gej_set_ge(r, b); */
/*         return; */
/*     } */

/*     if (b.infinity != 0) { */
/*         if (rzr != NULL) { */
/*             secp256k1_fe_set_int(rzr, 1); */
/*         } */
/*         r = a; */
/*         return; */
/*     } */

/*     r.infinity = 0; */
/*     secp256k1_fe_sqr(z12, a.z); */
/*     u1 = a.x; secp256k1_fe_normalize_weak(u1); */
/*     secp256k1_fe_mul(u2, b.x, z12); */
/*     s1 = a.y; secp256k1_fe_normalize_weak(s1); */
/*     secp256k1_fe_mul(s2, b.y, z12); secp256k1_fe_mul(s2, s2, a.z); */
/*     secp256k1_fe_negate(h, u1, 1); secp256k1_fe_add(h, u2); */
/*     secp256k1_fe_negate(i, s1, 1); secp256k1_fe_add(i, s2); */
/*     if (secp256k1_fe_normalizes_to_zero_var(h)) { */
/*         if (secp256k1_fe_normalizes_to_zero_var(i)) { */
/*             secp256k1_gej_double_var(r, a, rzr); */
/*         } else { */
/*             if (rzr != NULL) { */
/*                 secp256k1_fe_set_int(rzr, 0); */
/*             } */
/*             r.infinity = 1; */
/*         } */
/*         return; */
/*     } */
/*     secp256k1_fe_sqr(i2, i); */
/*     secp256k1_fe_sqr(h2, h); */
/*     secp256k1_fe_mul(h3, h, h2); */
/*     if (rzr != NULL) { */
/*         rzr = h; */
/*     } */
/*     secp256k1_fe_mul(r.z, a.z, h); */
/*     secp256k1_fe_mul(t, u1, h2); */
/*     r.x = t; secp256k1_fe_mul_int(r.x, 2); secp256k1_fe_add(r.x, h3); */
/*     secp256k1_fe_negate(r.x, r.x, 3); secp256k1_fe_add(r.x, i2); */
/*     secp256k1_fe_negate(r.y, r.x, 5); secp256k1_fe_add(r.y, t); */
/*     secp256k1_fe_mul(r.y, r.y, i); */
/*     secp256k1_fe_mul(h3, h3, s1); secp256k1_fe_negate(h3, h3, 1); */
/*     secp256k1_fe_add(r.y, h3); */
/* } */

void secp256k1_fe_sqr_inner(in out uint[10] r, in const uint[10] a) {
    uint64_t c, d;
    uint64_t u0, u1, u2, u3, u4, u5, u6, u7, u8;
    uint t9, t0, t1, t2, t3, t4, t5, t6, t7;
    const uint64_t M = 0x3FFFFFF, R0 = 0x3D10, R1 = 0x400;

    d  = uint64_t(a[0]*2) * a[9]
       + uint64_t(a[1]*2) * a[8]
       + uint64_t(a[2]*2) * a[7]
       + uint64_t(a[3]*2) * a[6]
       + uint64_t(a[4]*2) * a[5];
    t9 = uint(d & M); d >>= 26;
    c  = uint64_t(a[0]) * a[0];
    d += uint64_t(a[1]*2) * a[9]
       + uint64_t(a[2]*2) * a[8]
       + uint64_t(a[3]*2) * a[7]
       + uint64_t(a[4]*2) * a[6]
       + uint64_t(a[5]) * a[5];
    u0 = uint(d & M); d >>= 26; c += u0 * R0;
    t0 = uint(c & M); c >>= 26; c += u0 * R1;

    c += uint64_t(a[0]*2) * a[1];
    d += uint64_t(a[2]*2) * a[9]
       + uint64_t(a[3]*2) * a[8]
       + uint64_t(a[4]*2) * a[7]
       + uint64_t(a[5]*2) * a[6];
    u1 = uint(d & M); d >>= 26; c += u1 * R0;
    t1 = uint(c & M); c >>= 26; c += u1 * R1;

    c += uint64_t(a[0]*2) * a[2]
       + uint64_t(a[1]) * a[1];
    d += uint64_t(a[3]*2) * a[9]
       + uint64_t(a[4]*2) * a[8]
       + uint64_t(a[5]*2) * a[7]
       + uint64_t(a[6]) * a[6];
    u2 = uint(d & M); d >>= 26; c += u2 * R0;
    t2 = uint(c & M); c >>= 26; c += u2 * R1;
    c += uint64_t(a[0]*2) * a[3]
       + uint64_t(a[1]*2) * a[2];
    d += uint64_t(a[4]*2) * a[9]
       + uint64_t(a[5]*2) * a[8]
       + uint64_t(a[6]*2) * a[7];
    u3 = uint(d & M); d >>= 26; c += u3 * R0;
    t3 = uint(c & M); c >>= 26; c += u3 * R1;
    c += uint64_t(a[0]*2) * a[4]
       + uint64_t(a[1]*2) * a[3]
       + uint64_t(a[2]) * a[2];
    d += uint64_t(a[5]*2) * a[9]
       + uint64_t(a[6]*2) * a[8]
       + uint64_t(a[7]) * a[7];
    u4 = uint(d & M); d >>= 26; c += u4 * R0;
    t4 = uint(c & M); c >>= 26; c += u4 * R1;
    c += uint64_t(a[0]*2) * a[5]
       + uint64_t(a[1]*2) * a[4]
       + uint64_t(a[2]*2) * a[3];
    d += uint64_t(a[6]*2) * a[9]
       + uint64_t(a[7]*2) * a[8];
    u5 = uint(d & M); d >>= 26; c += u5 * R0;
    t5 = uint(c & M); c >>= 26; c += u5 * R1;
    c += uint64_t(a[0]*2) * a[6]
       + uint64_t(a[1]*2) * a[5]
       + uint64_t(a[2]*2) * a[4]
       + uint64_t(a[3]) * a[3];
    d += uint64_t(a[7]*2) * a[9]
       + uint64_t(a[8]) * a[8];
    u6 = uint(d & M); d >>= 26; c += u6 * R0;
    t6 = uint(c & M); c >>= 26; c += u6 * R1;
    c += uint64_t(a[0]*2) * a[7]
       + uint64_t(a[1]*2) * a[6]
       + uint64_t(a[2]*2) * a[5]
       + uint64_t(a[3]*2) * a[4];
    d += uint64_t(a[8]*2) * a[9];
    u7 = uint(d & M); d >>= 26; c += u7 * R0;
    t7 = uint(c & M); c >>= 26; c += u7 * R1;
    c += uint64_t(a[0]*2) * a[8]
       + uint64_t(a[1]*2) * a[7]
       + uint64_t(a[2]*2) * a[6]
       + uint64_t(a[3]*2) * a[5]
       + uint64_t(a[4]) * a[4];
    d += uint64_t(a[9]) * a[9];
    u8 = uint(d & M); d >>= 26; c += u8 * R0;
    r[3] = t3;
    r[4] = t4;
    r[5] = t5;
    r[6] = t6;
    r[7] = t7;
    r[8] = uint(c & M); c >>= 26; c += u8 * R1;
    c   += d * R0 + t9;
    r[9] = uint(c & (M >> 4)); c >>= 22; c += d * (R1 << 4);
    d    = c * (R0 >> 4) + t0;
    r[0] = uint(d & M); d >>= 26;
    d   += c * (R1 >> 4) + t1;
    r[1] = uint(d & M); d >>= 26;
    d   += t2;
    r[2] = uint(d);
}

void secp256k1_fe_sqr(in out secp256k1_fe r, in const secp256k1_fe a) {
    secp256k1_fe_sqr_inner(r.n, a.n);
}

void secp256k1_fe_normalize_weak(in out secp256k1_fe r) {
    uint t0 = r.n[0], t1 = r.n[1], t2 = r.n[2], t3 = r.n[3], t4 = r.n[4],
             t5 = r.n[5], t6 = r.n[6], t7 = r.n[7], t8 = r.n[8], t9 = r.n[9];

    /* Reduce t9 at the start so there will be at most a single carry from the first pass */
    uint x = t9 >> 22; t9 &= 0x03FFFFF;

    /* The first pass ensures the magnitude is 1, ... */
    t0 += x * 0x3D1; t1 += (x << 6);
    t1 += (t0 >> 26); t0 &= 0x3FFFFFF;
    t2 += (t1 >> 26); t1 &= 0x3FFFFFF;
    t3 += (t2 >> 26); t2 &= 0x3FFFFFF;
    t4 += (t3 >> 26); t3 &= 0x3FFFFFF;
    t5 += (t4 >> 26); t4 &= 0x3FFFFFF;
    t6 += (t5 >> 26); t5 &= 0x3FFFFFF;
    t7 += (t6 >> 26); t6 &= 0x3FFFFFF;
    t8 += (t7 >> 26); t7 &= 0x3FFFFFF;
    t9 += (t8 >> 26); t8 &= 0x3FFFFFF;

    r.n[0] = t0; r.n[1] = t1; r.n[2] = t2; r.n[3] = t3; r.n[4] = t4;
    r.n[5] = t5; r.n[6] = t6; r.n[7] = t7; r.n[8] = t8; r.n[9] = t9;
}

void secp256k1_fe_mul_inner(in out uint[10] r, in const uint[10] a, in const uint[10] b) {
    uint64_t c, d;
    uint64_t u0, u1, u2, u3, u4, u5, u6, u7, u8;
    uint t9, t1, t0, t2, t3, t4, t5, t6, t7;
    const uint64_t M = 0x3FFFFFF, R0 = 0x3D10, R1 = 0x400;

    d  = uint64_t(a[0]) * b[9]
       + uint64_t(a[1]) * b[8]
       + uint64_t(a[2]) * b[7]
       + uint64_t(a[3]) * b[6]
       + uint64_t(a[4]) * b[5]
       + uint64_t(a[5]) * b[4]
       + uint64_t(a[6]) * b[3]
       + uint64_t(a[7]) * b[2]
       + uint64_t(a[8]) * b[1]
       + uint64_t(a[9]) * b[0];
    t9 = uint(d & M); d >>= 26;

    c  = uint64_t(a[0]) * b[0];
    d += uint64_t(a[1]) * b[9]
       + uint64_t(a[2]) * b[8]
       + uint64_t(a[3]) * b[7]
       + uint64_t(a[4]) * b[6]
       + uint64_t(a[5]) * b[5]
       + uint64_t(a[6]) * b[4]
       + uint64_t(a[7]) * b[3]
       + uint64_t(a[8]) * b[2]
       + uint64_t(a[9]) * b[1];
    u0 = uint(d & M); d >>= 26; c += u0 * R0;
    t0 = uint(c & M); c >>= 26; c += u0 * R1;

    c += uint64_t(a[0]) * b[1]
       + uint64_t(a[1]) * b[0];
    d += uint64_t(a[2]) * b[9]
       + uint64_t(a[3]) * b[8]
       + uint64_t(a[4]) * b[7]
       + uint64_t(a[5]) * b[6]
       + uint64_t(a[6]) * b[5]
       + uint64_t(a[7]) * b[4]
       + uint64_t(a[8]) * b[3]
       + uint64_t(a[9]) * b[2];
    u1 = uint(d & M); d >>= 26; c += u1 * R0;
    t1 = uint(c & M); c >>= 26; c += u1 * R1;

    c += uint64_t(a[0]) * b[2]
       + uint64_t(a[1]) * b[1]
       + uint64_t(a[2]) * b[0];
    d += uint64_t(a[3]) * b[9]
       + uint64_t(a[4]) * b[8]
       + uint64_t(a[5]) * b[7]
       + uint64_t(a[6]) * b[6]
       + uint64_t(a[7]) * b[5]
       + uint64_t(a[8]) * b[4]
       + uint64_t(a[9]) * b[3];
    u2 = uint(d & M); d >>= 26; c += u2 * R0;
    t2 = uint(c & M); c >>= 26; c += u2 * R1;

    c += uint64_t(a[0]) * b[3]
       + uint64_t(a[1]) * b[2]
       + uint64_t(a[2]) * b[1]
       + uint64_t(a[3]) * b[0];
    d += uint64_t(a[4]) * b[9]
       + uint64_t(a[5]) * b[8]
       + uint64_t(a[6]) * b[7]
       + uint64_t(a[7]) * b[6]
       + uint64_t(a[8]) * b[5]
       + uint64_t(a[9]) * b[4];
    u3 = uint(d & M); d >>= 26; c += u3 * R0;
    t3 = uint(c & M); c >>= 26; c += u3 * R1;

    c += uint64_t(a[0]) * b[4]
       + uint64_t(a[1]) * b[3]
       + uint64_t(a[2]) * b[2]
       + uint64_t(a[3]) * b[1]
       + uint64_t(a[4]) * b[0];
    d += uint64_t(a[5]) * b[9]
       + uint64_t(a[6]) * b[8]
       + uint64_t(a[7]) * b[7]
       + uint64_t(a[8]) * b[6]
       + uint64_t(a[9]) * b[5];
    u4 = uint(d & M); d >>= 26; c += u4 * R0;
    t4 = uint(c & M); c >>= 26; c += u4 * R1;

    c += uint64_t(a[0]) * b[5]
       + uint64_t(a[1]) * b[4]
       + uint64_t(a[2]) * b[3]
       + uint64_t(a[3]) * b[2]
       + uint64_t(a[4]) * b[1]
       + uint64_t(a[5]) * b[0];
    d += uint64_t(a[6]) * b[9]
       + uint64_t(a[7]) * b[8]
       + uint64_t(a[8]) * b[7]
       + uint64_t(a[9]) * b[6];
    u5 = uint(d & M); d >>= 26; c += u5 * R0;
    t5 = uint(c & M); c >>= 26; c += u5 * R1;

    c += uint64_t(a[0]) * b[6]
       + uint64_t(a[1]) * b[5]
       + uint64_t(a[2]) * b[4]
       + uint64_t(a[3]) * b[3]
       + uint64_t(a[4]) * b[2]
       + uint64_t(a[5]) * b[1]
       + uint64_t(a[6]) * b[0];
    d += uint64_t(a[7]) * b[9]
       + uint64_t(a[8]) * b[8]
       + uint64_t(a[9]) * b[7];
    u6 = uint(d & M); d >>= 26; c += u6 * R0;
    t6 = uint(c & M); c >>= 26; c += u6 * R1;

    c += uint64_t(a[0]) * b[7]
       + uint64_t(a[1]) * b[6]
       + uint64_t(a[2]) * b[5]
       + uint64_t(a[3]) * b[4]
       + uint64_t(a[4]) * b[3]
       + uint64_t(a[5]) * b[2]
       + uint64_t(a[6]) * b[1]
       + uint64_t(a[7]) * b[0];
    d += uint64_t(a[8]) * b[9]
       + uint64_t(a[9]) * b[8];
    u7 = uint(d & M); d >>= 26; c += u7 * R0;
    t7 = uint(c & M); c >>= 26; c += u7 * R1;

    c += uint64_t(a[0]) * b[8]
       + uint64_t(a[1]) * b[7]
       + uint64_t(a[2]) * b[6]
       + uint64_t(a[3]) * b[5]
       + uint64_t(a[4]) * b[4]
       + uint64_t(a[5]) * b[3]
       + uint64_t(a[6]) * b[2]
       + uint64_t(a[7]) * b[1]
       + uint64_t(a[8]) * b[0];
    d += uint64_t(a[9]) * b[9];
    u8 = uint(d & M); d >>= 26; c += u8 * R0;

    r[3] = t3;
    r[4] = t4;
    r[5] = t5;
    r[6] = t6;
    r[7] = t7;

    r[8] = uint(c & M); c >>= 26; c += u8 * R1;
    c   += d * R0 + t9;
    r[9] = uint(c & (M >> 4)); c >>= 22; c += d * (R1 << 4);

    d    = c * (R0 >> 4) + t0;
    r[0] = uint(d & M); d >>= 26;
    d   += c * (R1 >> 4) + t1;
    r[1] = uint(d & M); d >>= 26;
    d   += t2;
    r[2] = uint(d);
}

void secp256k1_fe_mul(in out secp256k1_fe r, in const secp256k1_fe a, in const secp256k1_fe b) {
    secp256k1_fe_mul_inner(r.n, a.n, b.n);
}

void secp256k1_fe_add(in out secp256k1_fe r, in const secp256k1_fe a) {
    r.n[0] += a.n[0];
    r.n[1] += a.n[1];
    r.n[2] += a.n[2];
    r.n[3] += a.n[3];
    r.n[4] += a.n[4];
    r.n[5] += a.n[5];
    r.n[6] += a.n[6];
    r.n[7] += a.n[7];
    r.n[8] += a.n[8];
    r.n[9] += a.n[9];
}

void secp256k1_fe_negate(in out secp256k1_fe r, in const secp256k1_fe a, int m) {
    r.n[0] = 0x3FFFC2F * 2 * (m + 1) - a.n[0];
    r.n[1] = 0x3FFFFBF * 2 * (m + 1) - a.n[1];
    r.n[2] = 0x3FFFFFF * 2 * (m + 1) - a.n[2];
    r.n[3] = 0x3FFFFFF * 2 * (m + 1) - a.n[3];
    r.n[4] = 0x3FFFFFF * 2 * (m + 1) - a.n[4];
    r.n[5] = 0x3FFFFFF * 2 * (m + 1) - a.n[5];
    r.n[6] = 0x3FFFFFF * 2 * (m + 1) - a.n[6];
    r.n[7] = 0x3FFFFFF * 2 * (m + 1) - a.n[7];
    r.n[8] = 0x3FFFFFF * 2 * (m + 1) - a.n[8];
    r.n[9] = 0x03FFFFF * 2 * (m + 1) - a.n[9];
}

bool secp256k1_fe_normalizes_to_zero(in out secp256k1_fe r) {
    uint t0 = r.n[0], t1 = r.n[1], t2 = r.n[2], t3 = r.n[3], t4 = r.n[4],
             t5 = r.n[5], t6 = r.n[6], t7 = r.n[7], t8 = r.n[8], t9 = r.n[9];

    /* z0 tracks a possible raw value of 0, z1 tracks a possible raw value of P */
    uint z0, z1;

    /* Reduce t9 at the start so there will be at most a single carry from the first pass */
    uint x = t9 >> 22; t9 &= 0x03FFFFF;

    /* The first pass ensures the magnitude is 1, ... */
    t0 += x * 0x3D1; t1 += (x << 6);
    t1 += (t0 >> 26); t0 &= 0x3FFFFFF; z0  = t0; z1  = t0 ^ 0x3D0;
    t2 += (t1 >> 26); t1 &= 0x3FFFFFF; z0 |= t1; z1 &= t1 ^ 0x40;
    t3 += (t2 >> 26); t2 &= 0x3FFFFFF; z0 |= t2; z1 &= t2;
    t4 += (t3 >> 26); t3 &= 0x3FFFFFF; z0 |= t3; z1 &= t3;
    t5 += (t4 >> 26); t4 &= 0x3FFFFFF; z0 |= t4; z1 &= t4;
    t6 += (t5 >> 26); t5 &= 0x3FFFFFF; z0 |= t5; z1 &= t5;
    t7 += (t6 >> 26); t6 &= 0x3FFFFFF; z0 |= t6; z1 &= t6;
    t8 += (t7 >> 26); t7 &= 0x3FFFFFF; z0 |= t7; z1 &= t7;
    t9 += (t8 >> 26); t8 &= 0x3FFFFFF; z0 |= t8; z1 &= t8;
                                         z0 |= t9; z1 &= t9 ^ 0x3C00000;

    return (z0 == 0) || (z1 == 0x3FFFFFF);
}

void secp256k1_fe_mul_int(in out secp256k1_fe r, in int a) {
    r.n[0] *= a;
    r.n[1] *= a;
    r.n[2] *= a;
    r.n[3] *= a;
    r.n[4] *= a;
    r.n[5] *= a;
    r.n[6] *= a;
    r.n[7] *= a;
    r.n[8] *= a;
    r.n[9] *= a;
}

void secp256k1_fe_cmov(in out secp256k1_fe r, in const secp256k1_fe a, int flag) {
    uint mask0, mask1;
    mask0 = flag + ~0;
    mask1 = ~mask0;
    r.n[0] = (r.n[0] & mask0) | (a.n[0] & mask1);
    r.n[1] = (r.n[1] & mask0) | (a.n[1] & mask1);
    r.n[2] = (r.n[2] & mask0) | (a.n[2] & mask1);
    r.n[3] = (r.n[3] & mask0) | (a.n[3] & mask1);
    r.n[4] = (r.n[4] & mask0) | (a.n[4] & mask1);
    r.n[5] = (r.n[5] & mask0) | (a.n[5] & mask1);
    r.n[6] = (r.n[6] & mask0) | (a.n[6] & mask1);
    r.n[7] = (r.n[7] & mask0) | (a.n[7] & mask1);
    r.n[8] = (r.n[8] & mask0) | (a.n[8] & mask1);
    r.n[9] = (r.n[9] & mask0) | (a.n[9] & mask1);
}

void secp256k1_fe_cmov(in out secp256k1_fe r, in const secp256k1_fe a, bool flag) {
    secp256k1_fe_cmov(r, a, int(flag));
}

void secp256k1_gej_add_ge(in out secp256k1_gej r, in const secp256k1_gej a, in const secp256k1_ge b) {
    /* Operations: 7 mul, 5 sqr, 4 normalize, 21 mul_int/add/negate/cmov */
    const secp256k1_fe fe_1 = SECP256K1_FE_CONST(0, 0, 0, 0, 0, 0, 0, 1);
    secp256k1_fe zz, u1, u2, s1, s2, t, tt, m, n, q, rr;
    secp256k1_fe m_alt, rr_alt;
    int infinity;
    bool degenerate;

    secp256k1_fe_sqr(zz, a.z);                       /* z = Z1^2 */
    u1 = a.x; secp256k1_fe_normalize_weak(u1);        /* u1 = U1 = X1*Z2^2 (1) */
    secp256k1_fe_mul(u2, b.x, zz);                  /* u2 = U2 = X2*Z1^2 (1) */
    s1 = a.y; secp256k1_fe_normalize_weak(s1);        /* s1 = S1 = Y1*Z2^3 (1) */
    secp256k1_fe_mul(s2, b.y, zz);                  /* s2 = Y2*Z1^2 (1) */
    secp256k1_fe_mul(s2, s2, a.z);                  /* s2 = S2 = Y2*Z1^3 (1) */
    t = u1; secp256k1_fe_add(t, u2);                  /* t = T = U1+U2 (2) */
    m = s1; secp256k1_fe_add(m, s2);                  /* m = M = S1+S2 (2) */
    secp256k1_fe_sqr(rr, t);                          /* rr = T^2 (1) */
    secp256k1_fe_negate(m_alt, u2, 1);                /* Malt = -X2*Z1^2 */
    secp256k1_fe_mul(tt, u1, m_alt);                 /* tt = -U1*U2 (2) */
    secp256k1_fe_add(rr, tt);                         /* rr = R = T^2-U1*U2 (3) */
    /** If lambda = R/M = 0/0 we have a problem (except in the "trivial"
     *  case that Z = z1z2 = 0, and this is special-cased later on). */
    degenerate = secp256k1_fe_normalizes_to_zero(m) &&
                 secp256k1_fe_normalizes_to_zero(rr);
    /* This only occurs when y1 == -y2 and x1^3 == x2^3, but x1 != x2.
     * This means either x1 == beta*x2 or beta*x1 == x2, where beta is
     * a nontrivial cube root of one. In either case, an alternate
     * non-indeterminate expression for lambda is (y1 - y2)/(x1 - x2),
     * so we set R/M equal to this. */
    rr_alt = s1;
    secp256k1_fe_mul_int(rr_alt, 2);       /* rr = Y1*Z2^3 - Y2*Z1^3 (2) */
    secp256k1_fe_add(m_alt, u1);          /* Malt = X1*Z2^2 - X2*Z1^2 */

    secp256k1_fe_cmov(rr_alt, rr, !degenerate);
    secp256k1_fe_cmov(m_alt, m, !degenerate);
    /* Now Ralt / Malt = lambda and is guaranteed not to be 0/0.
     * From here on out Ralt and Malt represent the numerator
     * and denominator of lambda; R and M represent the explicit
     * expressions x1^2 + x2^2 + x1x2 and y1 + y2. */
    secp256k1_fe_sqr(n, m_alt);                       /* n = Malt^2 (1) */
    secp256k1_fe_mul(q, n, t);                       /* q = Q = T*Malt^2 (1) */
    /* These two lines use the observation that either M == Malt or M == 0,
     * so M^3 * Malt is either Malt^4 (which is computed by squaring), or
     * zero (which is "computed" by cmov). So the cost is one squaring
     * versus two multiplications. */
    secp256k1_fe_sqr(n, n);
    secp256k1_fe_cmov(n, m, degenerate);              /* n = M^3 * Malt (2) */
    secp256k1_fe_sqr(t, rr_alt);                      /* t = Ralt^2 (1) */
    secp256k1_fe_mul(r.z, a.z, m_alt);             /* r->z = Malt*Z (1) */
    infinity = int(secp256k1_fe_normalizes_to_zero(r.z)) * (1 - int(a.infinity));
    secp256k1_fe_mul_int(r.z, 2);                     /* r->z = Z3 = 2*Malt*Z (2) */
    secp256k1_fe_negate(q, q, 1);                     /* q = -Q (2) */
    secp256k1_fe_add(t, q);                           /* t = Ralt^2-Q (3) */
    secp256k1_fe_normalize_weak(t);
    r.x = t;                                           /* r->x = Ralt^2-Q (1) */
    secp256k1_fe_mul_int(t, 2);                        /* t = 2*x3 (2) */
    secp256k1_fe_add(t, q);                           /* t = 2*x3 - Q: (4) */
    secp256k1_fe_mul(t, t, rr_alt);                  /* t = Ralt*(2*x3 - Q) (1) */
    secp256k1_fe_add(t, n);                           /* t = Ralt*(2*x3 - Q) + M^3*Malt (3) */
    secp256k1_fe_negate(r.y, t, 3);                  /* r->y = Ralt*(Q - 2x3) - M^3*Malt (4) */
    secp256k1_fe_normalize_weak(r.y);
    secp256k1_fe_mul_int(r.x, 4);                     /* r->x = X3 = 4*(Ralt^2-Q) */
    secp256k1_fe_mul_int(r.y, 4);                     /* r->y = Y3 = 4*Ralt*(Q - 2x3) - 4*M^3*Malt (4) */

    /** In case a->infinity == 1, replace r with (b->x, b->y, 1). */
    secp256k1_fe_cmov(r.x, b.x, a.infinity);
    secp256k1_fe_cmov(r.y, b.y, a.infinity);
    secp256k1_fe_cmov(r.z, fe_1, a.infinity);
    r.infinity = infinity;
}

void secp256k1_ecmult_gen(in secp256k1_ecmult_gen_context ctx, out secp256k1_gej r, in secp256k1_scalar gn) {
    secp256k1_ge add;
    secp256k1_ge_storage adds;
    secp256k1_scalar gnb;
    uint bits;
    int i, j;
    /* memset(&adds, 0, sizeof(adds)); */
    /* *r = ctx->initial; */
    r = ctx.initial;
    /* Blind scalar/point multiplication by computing (n-b)G + bG instead of nG. */
    secp256k1_scalar_add(gnb, gn, ctx.blind);
    add.infinity = 0;
    for (j = 0; j < 64; j++) {
        bits = secp256k1_scalar_get_bits(gnb, j * 4, 4);
        for (i = 0; i < 16; i++) {
            secp256k1_ge_storage_cmov(adds, ctx.prec[j][i], i == bits);
        }
        secp256k1_ge_from_storage(add, adds);
        secp256k1_gej_add_ge(r, r, add);
    }
    // disabled as it doesn't affect any returned values
    /* bits = 0; */
    /* secp256k1_ge_clear(add); */
    /* secp256k1_scalar_clear(gnb); */
}

// removed secret key checks, make sure to enter valid secret keys.
void secp256k1_ec_pubkey_create(
        in secp256k1_ecmult_gen_context ctx,
        in out secp256k1_scalar pubkey,
        in const secp256k1_scalar seckey) {
    secp256k1_gej pj;
    secp256k1_ge p;
    /* pubkey = {0, 0, 0, 0, 0, 0, 0, 0}; */

    secp256k1_ecmult_gen(ctx, pj, seckey);
    /* secp256k1_ge_set_gej(p, pj); */
    /* secp256k1_pubkey_save(pubkey, p); */
}
// }}}

// }}}

// local workgroup dimensions
layout(local_size_x = 1, local_size_y = 1, local_size_z = 1) in;

// We need to split the prec array because it's so large it cannot be
// transferred to the gpu as a uniform buffer.
layout(set = 0, binding = 0) uniform ContextBufferPartPrecQuarterFirst {
    secp256k1_ecmult_gen_context_part_prec_quarter prec_quarter;
} context_buffer_part_prec_quarter_first;

layout(set = 0, binding = 1) uniform ContextBufferPartPrecQuarterSecond {
    secp256k1_ecmult_gen_context_part_prec_quarter prec_quarter;
} context_buffer_part_prec_quarter_second;

layout(set = 0, binding = 2) uniform ContextBufferPartPrecQuarterThird {
    secp256k1_ecmult_gen_context_part_prec_quarter prec_quarter;
} context_buffer_part_prec_quarter_third;

layout(set = 0, binding = 3) uniform ContextBufferPartPrecQuarterFourth {
    secp256k1_ecmult_gen_context_part_prec_quarter prec_quarter;
} context_buffer_part_prec_quarter_fourth;

layout(set = 0, binding = 4) uniform ContextBufferPartRest {
    secp256k1_ecmult_gen_context_part_rest context_rest;
} context_buffer_part_rest;

layout(set = 0, binding = 5) buffer OutputData {
    uint array[];
} output_data;

secp256k1_context_struct construct_context() {
    secp256k1_ge_storage[64][16] prec;

    for (int i = 0; i < 16; i++) {
        prec[i     ] = context_buffer_part_prec_quarter_first.prec_quarter.array_quarter[i];
    }

    for (int i = 0; i < 16; i++) {
        prec[i + 16] = context_buffer_part_prec_quarter_second.prec_quarter.array_quarter[i];
    }

    for (int i = 0; i < 16; i++) {
        prec[i + 32] = context_buffer_part_prec_quarter_third.prec_quarter.array_quarter[i];
    }

    for (int i = 0; i < 16; i++) {
        prec[i + 48] = context_buffer_part_prec_quarter_fourth.prec_quarter.array_quarter[i];
    }

    return secp256k1_context_struct(
        secp256k1_ecmult_gen_context(
            prec,
            context_buffer_part_rest.context_rest.blind,
            context_buffer_part_rest.context_rest.initial
        )
    );
}

void main() {
    secp256k1_context_struct context = construct_context();

    /* secp256k1_scalar seckey = secp256k1_scalar(input_data.array[gl_GlobalInvocationID.x]); */
    secp256k1_scalar seckey = secp256k1_scalar(uint[8] ( 0, 0, 0, 0, 0, 0, 0, 0 ));

    if(!secp256k1_ec_seckey_verify(seckey)) {
        output_data.array[gl_GlobalInvocationID.x] = 0;
        return;
    }

    output_data.array[gl_GlobalInvocationID.x] = 1;
    secp256k1_scalar pubkey;

    // We assume success as we already checked the validity of the secret key.
    secp256k1_ec_pubkey_create(context.ctx, pubkey, seckey);
}
